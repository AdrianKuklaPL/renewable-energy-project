{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEERING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code requests annual data from the API, and stores it to a JSON file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import csv\n",
    "import pandas as pd\n",
    "api_key = 'JF2O7UqI4VR9xdWAZN4RvJa4ePXnF45ET3kFfdWL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, params):\n",
    "    all_data = []  \n",
    "    total_rows = 0  \n",
    "    has_more_data = True\n",
    "\n",
    "    while has_more_data:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # check for the \"incomplete return\" warning; should hit this warning still\n",
    "            if data.get('response', {}).get('warnings'):\n",
    "                print(data['response']['warnings'][0]['description'])\n",
    "\n",
    "            # append the data to the list\n",
    "            all_data.extend(data['response']['data'])\n",
    "            total_rows += len(data['response']['data'])\n",
    "\n",
    "            # if fewer than 5000 rows are returned, finished\n",
    "            if len(data['response']['data']) < 5000:\n",
    "                has_more_data = False\n",
    "            else:\n",
    "                # update  offset to get the next batch of data\n",
    "                params['offset'] += 5000\n",
    "\n",
    "            # delay to avoid API limit\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total rows retrieved: {total_rows}\")\n",
    "    return all_data\n",
    "def save_to_json(data, filename, array_name):\n",
    "     with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        print(f\"Data saved to {filename} as a list of JSON objects.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores the emissions from Energy Consumption at Conventional Power Plants and Co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "Total rows retrieved: 8571\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/emissions-by-state-by-fuel/data/'\n",
    "params = {\n",
    "    'api_key': api_key, \n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'co2-rate-lbs-mwh',\n",
    "    'data[1]': 'co2-thousand-metric-tons',\n",
    "    'data[2]': 'nox-rate-lbs-mwh',\n",
    "    'data[3]': 'nox-short-tons',\n",
    "    'data[4]': 'so2-rate-lbs-mwh',\n",
    "    'data[5]': 'so2-short-tons',\n",
    "    'facets[stateid][]': [\n",
    "        'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI',\n",
    "        'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN',\n",
    "        'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n",
    "        'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'US', 'UT', 'VA', 'VT',\n",
    "        'WA', 'WI', 'WV', 'WY'\n",
    "    ],\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000 \n",
    "}\n",
    "\n",
    "emissions_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores supply and disposition of electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows retrieved: 1768\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/source-disposition/data/'\n",
    "params = {\n",
    "    'api_key': api_key,  \n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'combined-heat-and-pwr-comm',\n",
    "    'data[1]': 'combined-heat-and-pwr-elect',\n",
    "    'data[2]': 'combined-heat-and-pwr-indust',\n",
    "    'data[3]': 'direct-use',\n",
    "    'data[4]': 'elect-pwr-sector-gen-subtotal',\n",
    "    'data[5]': 'electric-utilities',\n",
    "    'data[6]': 'energy-only-providers',\n",
    "    'data[7]': 'estimated-losses',\n",
    "    'data[8]': 'facility-direct',\n",
    "    'data[9]': 'full-service-providers',\n",
    "    'data[10]': 'independent-power-producers',\n",
    "    'data[11]': 'indust-and-comm-gen-subtotal',\n",
    "    'data[12]': 'net-interstate-trade',\n",
    "    'data[13]': 'net-trade-index',\n",
    "    'data[14]': 'total-disposition',\n",
    "    'data[15]': 'total-elect-indust',\n",
    "    'data[16]': 'total-international-exports',\n",
    "    'data[17]': 'total-international-imports',\n",
    "    'data[18]': 'total-net-generation',\n",
    "    'data[19]': 'total-supply',\n",
    "    'data[20]': 'unaccounted',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000  \n",
    "}\n",
    "source_disposition_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores net generation capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "Total rows retrieved: 54356\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/capability/data/'\n",
    "params = {\n",
    "    'api_key': api_key,  \n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'capability',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000 \n",
    "}\n",
    "capacity_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores the costs and savings for energy efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows retrieved: 4680\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/energy-efficiency/data/'\n",
    "\n",
    "params = {\n",
    "    'api_key': api_key,\n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'all-other-costs',\n",
    "    'data[1]': 'customer-incentive',\n",
    "    'data[2]': 'energy-savings',\n",
    "    'data[3]': 'potential-peak-savings',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000  \n",
    "}\n",
    "energy_efficiency_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores electricty net metering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "Total rows retrieved: 8320\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/net-metering/data/'\n",
    "params = {\n",
    "    'api_key': api_key, \n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'capacity',\n",
    "    'data[1]': 'customers',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000 \n",
    "}\n",
    "net_metering_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores advanced metering infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "The API can only return 5000 rows in JSON format.  Please consider constraining your request with facet, start, or end, or using offset to paginate results.\n",
      "Total rows retrieved: 8320\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/meters/data/'\n",
    "params = {\n",
    "    'api_key': api_key, \n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'meters',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000  \n",
    "}\n",
    "adv_metering_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table which stores state rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows retrieved: 815\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/state-electricity-profiles/summary/data/'\n",
    "params = {\n",
    "    'api_key': api_key,  \n",
    "    'frequency': 'annual',\n",
    "    'data[0]': 'average-retail-price',\n",
    "    'data[1]': 'average-retail-price-rank',\n",
    "    'data[2]': 'capacity-elec-utilities',\n",
    "    'data[3]': 'capacity-elect-utilities-rank',\n",
    "    'data[4]': 'capacity-ipp',\n",
    "    'data[5]': 'capacity-ipp-rank',\n",
    "    'data[6]': 'carbon-dioxide',\n",
    "    'data[7]': 'carbon-dioxide-lbs',\n",
    "    'data[8]': 'carbon-dioxide-rank',\n",
    "    'data[9]': 'carbon-dioxide-rank-lbs',\n",
    "    'data[10]': 'direct-use',\n",
    "    'data[11]': 'direct-use-rank',\n",
    "    'data[12]': 'eop-sales',\n",
    "    'data[13]': 'eop-sales-rank',\n",
    "    'data[14]': 'fsp-sales-rank',\n",
    "    'data[15]': 'fsp-service-provider-sales',\n",
    "    'data[16]': 'generation-elect-utils',\n",
    "    'data[17]': 'generation-elect-utils-rank',\n",
    "    'data[18]': 'generation-ipp',\n",
    "    'data[19]': 'generation-ipp-rank',\n",
    "    'data[20]': 'net-generation',\n",
    "    'data[21]': 'net-generation-rank',\n",
    "    'data[22]': 'net-summer-capacity',\n",
    "    'data[23]': 'net-summer-capacity-rank',\n",
    "    'data[24]': 'nitrogen-oxide',\n",
    "    'data[25]': 'nitrogen-oxide-lbs',\n",
    "    'data[26]': 'nitrogen-oxide-rank',\n",
    "    'data[27]': 'nitrogen-oxide-rank-lbs',\n",
    "    'data[28]': 'prime-source',\n",
    "    'data[29]': 'sulfer-dioxide',\n",
    "    'data[30]': 'sulfer-dioxide-lbs',\n",
    "    'data[31]': 'sulfer-dioxide-rank',\n",
    "    'data[32]': 'sulfer-dioxide-rank-lbs',\n",
    "    'data[33]': 'total-retail-sales',\n",
    "    'data[34]': 'total-retail-sales-rank',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000 \n",
    "}\n",
    "state_ranking_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves tables as JSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to emissions_data.json as a list of JSON objects.\n",
      "Data saved to source_disposition_data.json as a list of JSON objects.\n",
      "Data saved to capacity_data.json as a list of JSON objects.\n",
      "Data saved to energy_efficiency_data.json as a list of JSON objects.\n",
      "Data saved to net_metering_data.json as a list of JSON objects.\n",
      "Data saved to adv_metering_data.json as a list of JSON objects.\n",
      "Data saved to state_ranking_data.json as a list of JSON objects.\n"
     ]
    }
   ],
   "source": [
    "save_to_json(emissions_data, 'emissions_data.json', 'emissions_data')\n",
    "save_to_json(source_disposition_data, 'source_disposition_data.json', 'source_disposition_data')\n",
    "save_to_json(capacity_data, 'capacity_data.json', 'capacity_data')\n",
    "save_to_json(energy_efficiency_data, 'energy_efficiency_data.json', 'energy_efficiency_data')\n",
    "save_to_json(net_metering_data, 'net_metering_data.json', 'net_metering_data')\n",
    "save_to_json(meters_data, 'adv_metering_data.json', 'meters_data')\n",
    "save_to_json(state_ranking_data, 'state_ranking_data.json', 'state_ranking_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code for saving the above JSON files to a MYSQL database stored on GCP server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Annual Emmissions Data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS emissions_data (\n",
    "    period VARCHAR(10),\n",
    "    stateid VARCHAR(10),\n",
    "    stateDescription VARCHAR(50),\n",
    "    fuelid VARCHAR(10),\n",
    "    fuelDescription VARCHAR(50),\n",
    "    co2_rate_lbs_mwh DECIMAL(10, 2),\n",
    "    co2_thousand_metric_tons DECIMAL(10, 2),\n",
    "    nox_rate_lbs_mwh DECIMAL(10, 2),\n",
    "    nox_short_tons DECIMAL(10, 2),\n",
    "    so2_rate_lbs_mwh DECIMAL(10, 2),\n",
    "    so2_short_tons DECIMAL(10, 2),\n",
    "    co2_rate_lbs_mwh_units VARCHAR(50),\n",
    "    co2_thousand_metric_tons_units VARCHAR(50),\n",
    "    nox_rate_lbs_mwh_units VARCHAR(50),\n",
    "    nox_short_tons_units VARCHAR(50),\n",
    "    so2_rate_lbs_mwh_units VARCHAR(50),\n",
    "    so2_short_tons_units VARCHAR(50)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "with open('emissions_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO emissions_data (\n",
    "    period, stateid, stateDescription, fuelid, fuelDescription,\n",
    "    co2_rate_lbs_mwh, co2_thousand_metric_tons, nox_rate_lbs_mwh, nox_short_tons,\n",
    "    so2_rate_lbs_mwh, so2_short_tons, co2_rate_lbs_mwh_units, co2_thousand_metric_tons_units,\n",
    "    nox_rate_lbs_mwh_units, nox_short_tons_units, so2_rate_lbs_mwh_units, so2_short_tons_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"stateid\"),\n",
    "        record.get(\"stateDescription\"),\n",
    "        record.get(\"fuelid\"),\n",
    "        record.get(\"fuelDescription\"),\n",
    "        float(record.get(\"co2-rate-lbs-mwh\") or 0),\n",
    "        float(record.get(\"co2-thousand-metric-tons\") or 0),\n",
    "        float(record.get(\"nox-rate-lbs-mwh\") or 0),\n",
    "        float(record.get(\"nox-short-tons\") or 0),\n",
    "        float(record.get(\"so2-rate-lbs-mwh\") or 0),\n",
    "        float(record.get(\"so2-short-tons\") or 0),\n",
    "        record.get(\"co2-rate-lbs-mwh-units\"),\n",
    "        record.get(\"co2-thousand-metric-tons-units\"),\n",
    "        record.get(\"nox-rate-lbs-mwh-units\"),\n",
    "        record.get(\"nox-short-tons-units\"),\n",
    "        record.get(\"so2-rate-lbs-mwh-units\"),\n",
    "        record.get(\"so2-short-tons-units\")\n",
    "    ))\n",
    "\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Supply and disposition of electricity data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS source_disposition_data (\n",
    "    period VARCHAR(10),\n",
    "    state VARCHAR(5),\n",
    "    stateDescription VARCHAR(50),\n",
    "    combined_heat_and_pwr_comm DECIMAL(15, 2),\n",
    "    combined_heat_and_pwr_elect DECIMAL(15, 2),\n",
    "    combined_heat_and_pwr_indust DECIMAL(15, 2),\n",
    "    direct_use DECIMAL(15, 2),\n",
    "    elect_pwr_sector_gen_subtotal DECIMAL(15, 2),\n",
    "    electric_utilities DECIMAL(15, 2),\n",
    "    energy_only_providers DECIMAL(15, 2),\n",
    "    estimated_losses DECIMAL(15, 2),\n",
    "    facility_direct DECIMAL(15, 2),\n",
    "    full_service_providers DECIMAL(15, 2),\n",
    "    independent_power_producers DECIMAL(15, 2),\n",
    "    indust_and_comm_gen_subtotal DECIMAL(15, 2),\n",
    "    net_interstate_trade DECIMAL(15, 2),\n",
    "    net_trade_index DECIMAL(5, 2),\n",
    "    total_disposition DECIMAL(15, 2),\n",
    "    total_elect_indust DECIMAL(15, 2),\n",
    "    total_international_exports DECIMAL(15, 2),\n",
    "    total_international_imports DECIMAL(15, 2),\n",
    "    total_net_generation DECIMAL(15, 2),\n",
    "    total_supply DECIMAL(15, 2),\n",
    "    unaccounted DECIMAL(15, 2),\n",
    "    combined_heat_and_pwr_comm_units VARCHAR(50),\n",
    "    combined_heat_and_pwr_elect_units VARCHAR(50),\n",
    "    combined_heat_and_pwr_indust_units VARCHAR(50),\n",
    "    direct_use_units VARCHAR(50),\n",
    "    elect_pwr_sector_gen_subtotal_units VARCHAR(50),\n",
    "    electric_utilities_units VARCHAR(50),\n",
    "    energy_only_providers_units VARCHAR(50),\n",
    "    estimated_losses_units VARCHAR(50),\n",
    "    facility_direct_units VARCHAR(50),\n",
    "    full_service_providers_units VARCHAR(50),\n",
    "    independent_power_producers_units VARCHAR(50),\n",
    "    indust_and_comm_gen_subtotal_units VARCHAR(50),\n",
    "    net_interstate_trade_units VARCHAR(50),\n",
    "    net_trade_index_units VARCHAR(50),\n",
    "    total_disposition_units VARCHAR(50),\n",
    "    total_elect_indust_units VARCHAR(50),\n",
    "    total_international_exports_units VARCHAR(50),\n",
    "    total_international_imports_units VARCHAR(50),\n",
    "    total_net_generation_units VARCHAR(50),\n",
    "    total_supply_units VARCHAR(50),\n",
    "    unaccounted_units VARCHAR(50)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "with open('source_disposition_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO source_disposition_data (\n",
    "    period, state, stateDescription, combined_heat_and_pwr_comm, combined_heat_and_pwr_elect,\n",
    "    combined_heat_and_pwr_indust, direct_use, elect_pwr_sector_gen_subtotal, electric_utilities,\n",
    "    energy_only_providers, estimated_losses, facility_direct, full_service_providers,\n",
    "    independent_power_producers, indust_and_comm_gen_subtotal, net_interstate_trade, net_trade_index,\n",
    "    total_disposition, total_elect_indust, total_international_exports, total_international_imports,\n",
    "    total_net_generation, total_supply, unaccounted, combined_heat_and_pwr_comm_units,\n",
    "    combined_heat_and_pwr_elect_units, combined_heat_and_pwr_indust_units, direct_use_units,\n",
    "    elect_pwr_sector_gen_subtotal_units, electric_utilities_units, energy_only_providers_units,\n",
    "    estimated_losses_units, facility_direct_units, full_service_providers_units,\n",
    "    independent_power_producers_units, indust_and_comm_gen_subtotal_units, net_interstate_trade_units,\n",
    "    net_trade_index_units, total_disposition_units, total_elect_indust_units,\n",
    "    total_international_exports_units, total_international_imports_units, total_net_generation_units,\n",
    "    total_supply_units, unaccounted_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"state\"),\n",
    "        record.get(\"stateDescription\"),\n",
    "        float(record.get(\"combined-heat-and-pwr-comm\") or 0),\n",
    "        float(record.get(\"combined-heat-and-pwr-elect\") or 0),\n",
    "        float(record.get(\"combined-heat-and-pwr-indust\") or 0),\n",
    "        float(record.get(\"direct-use\") or 0),\n",
    "        float(record.get(\"elect-pwr-sector-gen-subtotal\") or 0),\n",
    "        float(record.get(\"electric-utilities\") or 0),\n",
    "        float(record.get(\"energy-only-providers\") or 0),\n",
    "        float(record.get(\"estimated-losses\") or 0),\n",
    "        float(record.get(\"facility-direct\") or 0),\n",
    "        float(record.get(\"full-service-providers\") or 0),\n",
    "        float(record.get(\"independent-power-producers\") or 0),\n",
    "        float(record.get(\"indust-and-comm-gen-subtotal\") or 0),\n",
    "        float(record.get(\"net-interstate-trade\") or 0),\n",
    "        float(record.get(\"net-trade-index\") or 0),\n",
    "        float(record.get(\"total-disposition\") or 0),\n",
    "        float(record.get(\"total-elect-indust\") or 0),\n",
    "        float(record.get(\"total-international-exports\") or 0),\n",
    "        float(record.get(\"total-international-imports\") or 0),\n",
    "        float(record.get(\"total-net-generation\") or 0),\n",
    "        float(record.get(\"total-supply\") or 0),\n",
    "        float(record.get(\"unaccounted\") or 0),\n",
    "        record.get(\"combined-heat-and-pwr-comm-units\"),\n",
    "        record.get(\"combined-heat-and-pwr-elect-units\"),\n",
    "        record.get(\"combined-heat-and-pwr-indust-units\"),\n",
    "        record.get(\"direct-use-units\"),\n",
    "        record.get(\"elect-pwr-sector-gen-subtotal-units\"),\n",
    "        record.get(\"electric-utilities-units\"),\n",
    "        record.get(\"energy-only-providers-units\"),\n",
    "        record.get(\"estimated-losses-units\"),\n",
    "        record.get(\"facility-direct-units\"),\n",
    "        record.get(\"full-service-providers-units\"),\n",
    "        record.get(\"independent-power-producers-units\"),\n",
    "        record.get(\"indust-and-comm-gen-subtotal-units\"),\n",
    "        record.get(\"net-interstate-trade-units\"),\n",
    "        record.get(\"net-trade-index-units\"),\n",
    "        record.get(\"total-disposition-units\"),\n",
    "        record.get(\"total-elect-indust-units\"),\n",
    "        record.get(\"total-international-exports-units\"),\n",
    "        record.get(\"total-international-imports-units\"),\n",
    "        record.get(\"total-net-generation-units\"),\n",
    "        record.get(\"total-supply-units\"),\n",
    "        record.get(\"unaccounted-units\")\n",
    "    ))\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Generating Capacity data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TABLE IF EXISTS capacity_data\")\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS capacity_data (\n",
    "    period VARCHAR(10),\n",
    "    stateId VARCHAR(10),\n",
    "    stateDescription VARCHAR(50),\n",
    "    producertypeid VARCHAR(10),\n",
    "    producerTypeDescription VARCHAR(50),\n",
    "    energysourceid VARCHAR(10), \n",
    "    energySourceDescription VARCHAR(50),\n",
    "    capability DECIMAL(12, 2),\n",
    "    capability_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "with open('capacity_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO capacity_data (\n",
    "    period, stateId, stateDescription, producertypeid, producerTypeDescription,\n",
    "    energysourceid, energySourceDescription, capability, capability_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    try:\n",
    "        energysourceid_value = str(record.get(\"energysourceid\", \"\")).strip()\n",
    "        print(f\"Inserting record with energysourceid: '{energysourceid_value}'\")\n",
    "        cursor.execute(insert_query, (\n",
    "            record.get(\"period\"),\n",
    "            record.get(\"stateId\"),\n",
    "            record.get(\"stateDescription\"),\n",
    "            record.get(\"producertypeid\"),\n",
    "            record.get(\"producerTypeDescription\"),\n",
    "            energysourceid_value,\n",
    "            record.get(\"energySourceDescription\"),\n",
    "            float(record.get(\"capability\") or 0),\n",
    "            record.get(\"capability-units\")\n",
    "        ))\n",
    "    #exepct size error  5?\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error inserting record: {record}\")\n",
    "        print(f\"MySQL Error: {err}\")\n",
    "    except ValueError as ve:\n",
    "        print(f\"Value error inserting record: {record}\")\n",
    "        print(f\"Python Error: {ve}\")\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Energy Efficiency Data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TABLE IF EXISTS energy_efficiency_data\")\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE energy_efficiency_data (\n",
    "    period VARCHAR(10),\n",
    "    state VARCHAR(5),\n",
    "    stateName VARCHAR(50),\n",
    "    timePeriod VARCHAR(50),\n",
    "    sector VARCHAR(10),\n",
    "    sectorName VARCHAR(50),\n",
    "    all_other_costs DECIMAL(15, 2),\n",
    "    customer_incentive DECIMAL(15, 2),\n",
    "    energy_savings DECIMAL(15, 2),\n",
    "    potential_peak_savings DECIMAL(10, 2),\n",
    "    all_other_costs_units VARCHAR(20),\n",
    "    customer_incentive_units VARCHAR(20),\n",
    "    energy_savings_units VARCHAR(20),\n",
    "    potential_peak_savings_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "with open('energy_efficiency_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO energy_efficiency_data (\n",
    "    period, state, stateName, timePeriod, sector, sectorName,\n",
    "    all_other_costs, customer_incentive, energy_savings, potential_peak_savings,\n",
    "    all_other_costs_units, customer_incentive_units, energy_savings_units, potential_peak_savings_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"state\"),\n",
    "        record.get(\"stateName\"),\n",
    "        record.get(\"timePeriod\"),\n",
    "        record.get(\"sector\"),\n",
    "        record.get(\"sectorName\"),\n",
    "        float(record.get(\"all-other-costs\") or 0),\n",
    "        float(record.get(\"customer-incentive\") or 0),\n",
    "        float(record.get(\"energy-savings\") or 0),\n",
    "        float(record.get(\"potential-peak-savings\") or 0),\n",
    "        record.get(\"all-other-costs-units\"),\n",
    "        record.get(\"customer-incentive-units\"),\n",
    "        record.get(\"energy-savings-units\"),\n",
    "        record.get(\"potential-peak-savings-units\")\n",
    "    ))\n",
    "\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Electricity Net Metering data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TABLE IF EXISTS net_metering_data\")\n",
    "\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE net_metering_data (\n",
    "    period VARCHAR(10),\n",
    "    state VARCHAR(5),\n",
    "    stateName VARCHAR(50),\n",
    "    technology VARCHAR(50),\n",
    "    sector VARCHAR(10),\n",
    "    sectorName VARCHAR(50),\n",
    "    capacity DECIMAL(12, 3),\n",
    "    customers INT,\n",
    "    capacity_units VARCHAR(20),\n",
    "    customers_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "with open('net_metering_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO net_metering_data (\n",
    "    period, state, stateName, technology, sector, sectorName,\n",
    "    capacity, customers, capacity_units, customers_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"state\"),\n",
    "        record.get(\"stateName\"),\n",
    "        record.get(\"technology\"),\n",
    "        record.get(\"sector\"),\n",
    "        record.get(\"sectorName\"),\n",
    "        float(record.get(\"capacity\") or 0),\n",
    "        int(record.get(\"customers\") or 0),\n",
    "        record.get(\"capacity-units\"),\n",
    "        record.get(\"customers-units\")\n",
    "    ))\n",
    "\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Advanced Metering data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_query = \"\"\"\n",
    "CREATE TABLE adv_metering_data (\n",
    "    period VARCHAR(10),\n",
    "    state VARCHAR(5),\n",
    "    stateName VARCHAR(50),\n",
    "    technology VARCHAR(50),\n",
    "    sector VARCHAR(10),\n",
    "    sectorName VARCHAR(50),\n",
    "    meters INT,\n",
    "    meters_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "with open('adv_metering_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO adv_metering_data (\n",
    "    period, state, stateName, technology, sector, sectorName,\n",
    "    meters, meters_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    cursor.execute(insert_query, (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"state\"),\n",
    "        record.get(\"stateName\"),\n",
    "        record.get(\"technology\"),\n",
    "        record.get(\"sector\"),\n",
    "        record.get(\"sectorName\"),\n",
    "        int(record.get(\"meters\") or 0),\n",
    "        record.get(\"meters-units\")\n",
    "    ))\n",
    "\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing State Ranking data in a MYSQL database on GCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DROP TABLE IF EXISTS state_ranking_data\")\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE state_ranking_data (\n",
    "    period VARCHAR(10),\n",
    "    stateID VARCHAR(5),\n",
    "    stateDescription VARCHAR(50),\n",
    "    average_retail_price DECIMAL(10, 2),\n",
    "    average_retail_price_rank INT,\n",
    "    capacity_elec_utilities DECIMAL(15, 2),\n",
    "    capacity_elec_utilities_rank INT,\n",
    "    capacity_ipp DECIMAL(15, 2),\n",
    "    capacity_ipp_rank INT,\n",
    "    carbon_dioxide DECIMAL(15, 2),\n",
    "    carbon_dioxide_lbs DECIMAL(10, 2),\n",
    "    carbon_dioxide_rank INT,\n",
    "    carbon_dioxide_rank_lbs INT,\n",
    "    direct_use DECIMAL(15, 2),\n",
    "    direct_use_rank INT,\n",
    "    eop_sales DECIMAL(15, 2),\n",
    "    eop_sales_rank INT,\n",
    "    fsp_sales_rank INT,\n",
    "    fsp_service_provider_sales DECIMAL(15, 2),\n",
    "    generation_elec_utils DECIMAL(15, 2),\n",
    "    generation_elec_utils_rank INT,\n",
    "    generation_ipp DECIMAL(15, 2),\n",
    "    generation_ipp_rank INT,\n",
    "    net_generation DECIMAL(15, 2),\n",
    "    net_generation_rank INT,\n",
    "    net_summer_capacity DECIMAL(15, 2),\n",
    "    net_summer_capacity_rank INT,\n",
    "    nitrogen_oxide DECIMAL(10, 2),\n",
    "    nitrogen_oxide_lbs DECIMAL(10, 2),\n",
    "    nitrogen_oxide_rank INT,\n",
    "    nitrogen_oxide_rank_lbs INT,\n",
    "    prime_source VARCHAR(30),  -- Increased length\n",
    "    sulfer_dioxide DECIMAL(10, 2),\n",
    "    sulfer_dioxide_lbs DECIMAL(10, 2),\n",
    "    sulfer_dioxide_rank INT,\n",
    "    sulfer_dioxide_rank_lbs INT,\n",
    "    total_retail_sales DECIMAL(15, 2),\n",
    "    total_retail_sales_rank INT,\n",
    "    average_retail_price_units VARCHAR(50),  -- Increased length\n",
    "    average_retail_price_rank_units VARCHAR(20),  -- Increased length\n",
    "    capacity_elec_utilities_units VARCHAR(50),  -- Increased length\n",
    "    capacity_elec_utilities_rank_units VARCHAR(20),  -- Increased length\n",
    "    capacity_ipp_units VARCHAR(50),  -- Increased length\n",
    "    capacity_ipp_rank_units VARCHAR(20),  -- Increased length\n",
    "    carbon_dioxide_units VARCHAR(50),  -- Increased length\n",
    "    carbon_dioxide_lbs_units VARCHAR(50),  -- Increased length\n",
    "    carbon_dioxide_rank_units VARCHAR(20),  -- Increased length\n",
    "    carbon_dioxide_rank_lbs_units VARCHAR(20),  -- Increased length\n",
    "    direct_use_units VARCHAR(50),  -- Increased length\n",
    "    direct_use_rank_units VARCHAR(20),  -- Increased length\n",
    "    eop_sales_units VARCHAR(50),  -- Increased length\n",
    "    eop_sales_rank_units VARCHAR(20),  -- Increased length\n",
    "    fsp_sales_rank_units VARCHAR(20),  -- Increased length\n",
    "    fsp_service_provider_sales_units VARCHAR(50),  -- Increased length\n",
    "    generation_elec_utils_units VARCHAR(50),  -- Increased length\n",
    "    generation_elec_utils_rank_units VARCHAR(20),  -- Increased length\n",
    "    generation_ipp_units VARCHAR(50),  -- Increased length\n",
    "    generation_ipp_rank_units VARCHAR(20),  -- Increased length\n",
    "    net_generation_units VARCHAR(50),  -- Increased length\n",
    "    net_generation_rank_units VARCHAR(20),  -- Increased length\n",
    "    net_summer_capacity_units VARCHAR(50),  -- Increased length\n",
    "    net_summer_capacity_rank_units VARCHAR(20),  -- Increased length\n",
    "    nitrogen_oxide_units VARCHAR(50),  -- Increased length\n",
    "    nitrogen_oxide_lbs_units VARCHAR(50),  -- Increased length\n",
    "    sulfer_dioxide_units VARCHAR(50),  -- Increased length\n",
    "    sulfer_dioxide_lbs_units VARCHAR(50),  -- Increased length\n",
    "    sulfer_dioxide_rank_units VARCHAR(20),  -- Increased length\n",
    "    sulfer_dioxide_rank_lbs_units VARCHAR(20),  -- Increased length\n",
    "    total_retail_sales_units VARCHAR(50),  -- Increased length\n",
    "    total_retail_sales_rank_units VARCHAR(20)  -- Increased length\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Load the JSON data for state_ranking_data\n",
    "with open('state_ranking_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Insert the data into the table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO state_ranking_data (\n",
    "    period, stateID, stateDescription, average_retail_price, average_retail_price_rank,\n",
    "    capacity_elec_utilities, capacity_elec_utilities_rank, capacity_ipp, capacity_ipp_rank,\n",
    "    carbon_dioxide, carbon_dioxide_lbs, carbon_dioxide_rank, carbon_dioxide_rank_lbs,\n",
    "    direct_use, direct_use_rank, eop_sales, eop_sales_rank, fsp_sales_rank,\n",
    "    fsp_service_provider_sales, generation_elec_utils, generation_elec_utils_rank, generation_ipp,\n",
    "    generation_ipp_rank, net_generation, net_generation_rank, net_summer_capacity,\n",
    "    net_summer_capacity_rank, nitrogen_oxide, nitrogen_oxide_lbs, nitrogen_oxide_rank,\n",
    "    nitrogen_oxide_rank_lbs, prime_source, sulfer_dioxide, sulfer_dioxide_lbs, sulfer_dioxide_rank,\n",
    "    sulfer_dioxide_rank_lbs, total_retail_sales, total_retail_sales_rank,\n",
    "    average_retail_price_units, average_retail_price_rank_units, capacity_elec_utilities_units,\n",
    "    capacity_elec_utilities_rank_units, capacity_ipp_units, capacity_ipp_rank_units,\n",
    "    carbon_dioxide_units, carbon_dioxide_lbs_units, carbon_dioxide_rank_units,\n",
    "    carbon_dioxide_rank_lbs_units, direct_use_units, direct_use_rank_units, eop_sales_units,\n",
    "    eop_sales_rank_units, fsp_sales_rank_units, fsp_service_provider_sales_units,\n",
    "    generation_elec_utils_units, generation_elec_utils_rank_units, generation_ipp_units,\n",
    "    generation_ipp_rank_units, net_generation_units, net_generation_rank_units,\n",
    "    net_summer_capacity_units, net_summer_capacity_rank_units, nitrogen_oxide_units,\n",
    "    nitrogen_oxide_lbs_units, sulfer_dioxide_units, sulfer_dioxide_lbs_units,\n",
    "    sulfer_dioxide_rank_units, sulfer_dioxide_rank_lbs_units, total_retail_sales_units,\n",
    "    total_retail_sales_rank_units\n",
    ") VALUES (\n",
    "    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,%s, %s, %s, %s\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "for record in data:\n",
    "    try:\n",
    "        cursor.execute(insert_query, (\n",
    "            record.get(\"period\"),\n",
    "            record.get(\"stateID\"),\n",
    "            record.get(\"stateDescription\"),\n",
    "            float(record.get(\"average-retail-price\") or 0),\n",
    "            int(record.get(\"average-retail-price-rank\") or 0),\n",
    "            float(record.get(\"capacity-elec-utilities\") or 0),\n",
    "            int(record.get(\"capacity-elect-utilities-rank\") or 0),\n",
    "            float(record.get(\"capacity-ipp\") or 0),\n",
    "            int(record.get(\"capacity-ipp-rank\") or 0),\n",
    "            float(record.get(\"carbon-dioxide\") or 0),\n",
    "            float(record.get(\"carbon-dioxide-lbs\") or 0),\n",
    "            int(record.get(\"carbon-dioxide-rank\") or 0),\n",
    "            int(record.get(\"carbon-dioxide-rank-lbs\") or 0),\n",
    "            float(record.get(\"direct-use\") or 0),\n",
    "            int(record.get(\"direct-use-rank\") or 0),\n",
    "            float(record.get(\"eop-sales\") or 0),\n",
    "            int(record.get(\"eop-sales-rank\") or 0),\n",
    "            int(record.get(\"fsp-sales-rank\") or 0),\n",
    "            float(record.get(\"fsp-service-provider-sales\") or 0),\n",
    "            float(record.get(\"generation-elect-utils\") or 0),\n",
    "            int(record.get(\"generation-elect-utils-rank\") or 0),\n",
    "            float(record.get(\"generation-ipp\") or 0),\n",
    "            int(record.get(\"generation-ipp-rank\") or 0),\n",
    "            float(record.get(\"net-generation\") or 0),\n",
    "            int(record.get(\"net-generation-rank\") or 0),\n",
    "            float(record.get(\"net-summer-capacity\") or 0),\n",
    "            int(record.get(\"net-summer-capacity-rank\") or 0),\n",
    "            float(record.get(\"nitrogen-oxide\") or 0),\n",
    "            float(record.get(\"nitrogen-oxide-lbs\") or 0),\n",
    "            int(record.get(\"nitrogen-oxide-rank\") or 0),\n",
    "            int(record.get(\"nitrogen-oxide-rank-lbs\") or 0),\n",
    "            record.get(\"prime-source\"),\n",
    "            float(record.get(\"sulfer-dioxide\") or 0),\n",
    "            float(record.get(\"sulfer-dioxide-lbs\") or 0),\n",
    "            int(record.get(\"sulfer-dioxide-rank\") or 0),\n",
    "            int(record.get(\"sulfer-dioxide-rank-lbs\") or 0),\n",
    "            float(record.get(\"total-retail-sales\") or 0),\n",
    "            int(record.get(\"total-retail-sales-rank\") or 0),\n",
    "            record.get(\"average-retail-price-units\"),\n",
    "            record.get(\"average-retail-price-rank-units\"),\n",
    "            record.get(\"capacity-elec-utilities-units\"),\n",
    "            record.get(\"capacity-elect-utilities-rank-units\"),\n",
    "            record.get(\"capacity-ipp-units\"),\n",
    "            record.get(\"capacity-ipp-rank-units\"),\n",
    "            record.get(\"carbon-dioxide-units\"),\n",
    "            record.get(\"carbon-dioxide-lbs-units\"),\n",
    "            record.get(\"carbon-dioxide-rank-units\"),\n",
    "            record.get(\"carbon-dioxide-rank-lbs-units\"),\n",
    "            record.get(\"direct-use-units\"),\n",
    "            record.get(\"direct-use-rank-units\"),\n",
    "            record.get(\"eop-sales-units\"),\n",
    "            record.get(\"eop-sales-rank-units\"),\n",
    "            record.get(\"fsp-sales-rank-units\"),\n",
    "            record.get(\"fsp-service-provider-sales-units\"),\n",
    "            record.get(\"generation-elect-utils-units\"),\n",
    "            record.get(\"generation-elect-utils-rank-units\"),\n",
    "            record.get(\"generation-ipp-units\"),\n",
    "            record.get(\"generation-ipp-rank-units\"),\n",
    "            record.get(\"net-generation-units\"),\n",
    "            record.get(\"net-generation-rank-units\"),\n",
    "            record.get(\"net-summer-capacity-units\"),\n",
    "            record.get(\"net-summer-capacity-rank-units\"),\n",
    "            record.get(\"nitrogen-oxide-units\"),\n",
    "            record.get(\"nitrogen-oxide-lbs-units\"),\n",
    "            record.get(\"sulfer-dioxide-units\"),\n",
    "            record.get(\"sulfer-dioxide-lbs-units\"),\n",
    "            record.get(\"sulfer-dioxide-rank-units\"),\n",
    "            record.get(\"sulfer-dioxide-rank-lbs-units\"),\n",
    "            record.get(\"total-retail-sales-units\"),\n",
    "            record.get(\"total-retail-sales-rank-units\")\n",
    "        ))\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == 1406:  \n",
    "            print(f\"Data too long error for record: {record}\")\n",
    "        else:\n",
    "            print(f\"Error inserting record: {record}\")\n",
    "            print(f\"MySQL Error: {err}\")\n",
    "\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code requests daily data from the API, which is then structured as a JSON file to upload to MYSQL stored on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions used for storing data to MYSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, params):\n",
    "    all_data = []  \n",
    "    total_rows = 0  \n",
    "    has_more_data = True\n",
    "\n",
    "    while has_more_data:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # check for the \"incomplete return\" warning; should hit this warning still\n",
    "            if data.get('response', {}).get('warnings'):\n",
    "                print(data['response']['warnings'][0]['description'])\n",
    "\n",
    "            # append the data to the list\n",
    "            all_data.extend(data['response']['data'])\n",
    "            total_rows += len(data['response']['data'])\n",
    "\n",
    "            # if fewer than 5000 rows are returned, finished\n",
    "            if len(data['response']['data']) < 5000:\n",
    "                has_more_data = False\n",
    "            else:\n",
    "                # update  offset to get the next batch of data\n",
    "                params['offset'] += 5000\n",
    "            print(total_rows)\n",
    "            # delay to avoid API limit\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total rows retrieved: {total_rows}\")\n",
    "    return all_data\n",
    "def save_to_json(data, filename, array_name):\n",
    "     with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        print(f\"Data saved to {filename} as a list of JSON objects.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import json\n",
    "def create_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establishing the connection\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            password=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "        if connection.is_connected():\n",
    "            print(\"Connection to MySQL DB successful\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "    \n",
    "    return connection\n",
    "\n",
    "def close_connection(connection):\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"The connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL DB successful\n",
      "Connected to database: ('testing',)\n"
     ]
    }
   ],
   "source": [
    "#DO NOT UNCOMMENT; WILL AFFECT ENTIRE DB:\n",
    "# host_name = \"34.23.64.152\" \n",
    "# user_name = \"root\"  \n",
    "# user_password = \"cse6242\" \n",
    "# db_name = \"testing\" \n",
    "connection = create_connection(host_name, user_name, user_password, db_name)\n",
    "if connection:\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT DATABASE();\")\n",
    "    result = cursor.fetchone()\n",
    "    print(f\"Connected to database: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Daily Demand, Forecast, Generation, Interchange Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API URL\n",
    "url = 'https://api.eia.gov/v2/electricity/rto/daily-region-data/data/'\n",
    "\n",
    "# Set up the parameters with your API key and offset initialization\n",
    "params = {\n",
    "    'api_key': api_key,  # Replace 'YOUR_API_KEY' with your actual API key\n",
    "    'frequency': 'daily',\n",
    "    'data[0]': 'value',\n",
    "    'facets[respondent][]': [\n",
    "        'AEC', 'AECI', 'AVA', 'AVRN', 'AZPS', 'BANC', 'BPAT', 'CAL', 'CAR', 'CENT', 'CHPD',\n",
    "        'CISO', 'CPLE', 'CPLW', 'DEAA', 'DOPD', 'DUK', 'EEI', 'EPE', 'ERCO', 'FLA', 'FMPP',\n",
    "        'FPC', 'FPL', 'GCPD', 'GLHB', 'GRID', 'GRIF', 'GVL', 'GWA', 'HGMA', 'HST', 'IID',\n",
    "        'IPCO', 'ISNE', 'JEA', 'LDWP', 'LGEE', 'MIDA', 'MIDW', 'MISO', 'NE', 'NEVP', 'NSB',\n",
    "        'NW', 'NWMT', 'NY', 'NYIS', 'PACE', 'PACW', 'PGE', 'PJM', 'PNM', 'PSCO', 'PSEI',\n",
    "        'SC', 'SCEG', 'SCL', 'SE', 'SEC', 'SEPA', 'SOCO', 'SPA', 'SRP', 'SW', 'SWPP', 'TAL',\n",
    "        'TEC', 'TEN', 'TEPC', 'TEX', 'TIDC', 'TPWR', 'TVA', 'US48', 'WACM', 'WALC', 'WAUW',\n",
    "        'WWA', 'YAD'\n",
    "    ],\n",
    "    'facets[timezone][]': 'Eastern',\n",
    "    'sort[0][column]': 'respondent',\n",
    "    'sort[0][direction]': 'asc',\n",
    "    'sort[1][column]': 'period',\n",
    "    'sort[1][direction]': 'asc',\n",
    "    'sort[2][column]': 'type',\n",
    "    'sort[2][direction]': 'asc',\n",
    "    'offset': 0,\n",
    "    'length': 5000\n",
    "}\n",
    "hrly_data = fetch_data(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to hrly_data.json as a list of JSON objects.\n"
     ]
    }
   ],
   "source": [
    "save_to_json(hrly_data, 'daily_demand_data.json', 'daily_demand_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Daily Demand By Subregion Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/rto/daily-region-sub-ba-data/data/'\n",
    "\n",
    "# Set up the base parameters\n",
    "params = {\n",
    "    'api_key': api_key,  # Replace with your actual API key\n",
    "    'frequency': 'daily',\n",
    "    'data[0]': 'value',\n",
    "    'facets[timezone][]': 'Eastern',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000\n",
    "}\n",
    "\n",
    "# Fetch data using the defined function\n",
    "try:\n",
    "    daily_subregion_data = fetch_data(url, params)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to daily_subregion_data.json as a list of JSON objects.\n"
     ]
    }
   ],
   "source": [
    "save_to_json(daily_subregion_data, 'daily_subregion_data.json', 'daily_subregion_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Daily Generation by energy Source Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API URL for daily fuel type data\n",
    "url = 'https://api.eia.gov/v2/electricity/rto/daily-fuel-type-data/data/'\n",
    "\n",
    "# Set up the parameters for the API request\n",
    "params = {\n",
    "    'api_key': api_key,\n",
    "    'frequency': 'daily',\n",
    "    'data[0]': 'value',\n",
    "    'facets[timezone][]': 'Eastern',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000\n",
    "}\n",
    "\n",
    "# Fetch data using the defined function\n",
    "try:\n",
    "    daily_generation_data = fetch_data(url, params)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to daily_generation_data.json as a list of JSON objects.\n"
     ]
    }
   ],
   "source": [
    "save_to_json(daily_generation_data, 'daily_generation_data.json', 'daily_generation_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Daily Interchange Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.eia.gov/v2/electricity/rto/daily-interchange-data/data/'\n",
    "\n",
    "# Set up the parameters for the API request\n",
    "params = {\n",
    "    'api_key': api_key,\n",
    "    'frequency': 'daily',\n",
    "    'data[0]': 'value',\n",
    "    'facets[timezone][]': 'Eastern',\n",
    "    'sort[0][column]': 'period',\n",
    "    'sort[0][direction]': 'desc',\n",
    "    'offset': 0,\n",
    "    'length': 5000\n",
    "}\n",
    "\n",
    "# Fetch data using the defined function\n",
    "try:\n",
    "    interchange__nba_data = fetch_data(url, params)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to interchange_nba_data.json as a list of JSON objects.\n"
     ]
    }
   ],
   "source": [
    "save_to_json(interchange__nba_data, 'interchange_nba_data.json', 'interchange_nba_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Daily Data Tables into GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #daily demand data\n",
    "# Drop the table if it exists\n",
    "drop_table_query = \"DROP TABLE IF EXISTS daily_demand;\"\n",
    "cursor.execute(drop_table_query)\n",
    "\n",
    "# Create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_demand (\n",
    "    period VARCHAR(20),\n",
    "    respondent VARCHAR(20),\n",
    "    respondent_name VARCHAR(200),\n",
    "    type VARCHAR(20),\n",
    "    type_name VARCHAR(50),\n",
    "    timezone VARCHAR(20),\n",
    "    timezone_description VARCHAR(50),\n",
    "    value DECIMAL(20, 2),\n",
    "    value_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('daily_demand_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare the data for bulk insertion as a list of tuples\n",
    "insert_data = [\n",
    "    (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"respondent\"),\n",
    "        record.get(\"respondent-name\"),\n",
    "        record.get(\"type\"),\n",
    "        record.get(\"type-name\"),\n",
    "        record.get(\"timezone\"),\n",
    "        record.get(\"timezone-description\"),\n",
    "        float(record.get(\"value\") or 0),\n",
    "        record.get(\"value-units\")\n",
    "    )\n",
    "    for record in data\n",
    "]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 500  # Adjust this size as needed\n",
    "\n",
    "# Insert data in batches using executemany\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO daily_demand (\n",
    "    period, respondent, respondent_name, type, type_name, \n",
    "    timezone, timezone_description, value, value_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Initialize row counter\n",
    "row_ct = 0\n",
    "\n",
    "for i in range(0, len(insert_data), batch_size):\n",
    "    batch = insert_data[i:i + batch_size]\n",
    "    cursor.executemany(insert_query, batch)\n",
    "    connection.commit()  # Commit after each batch to avoid locking issues\n",
    "    \n",
    "    # Update and print the row count\n",
    "    row_ct += len(batch)\n",
    "    print(f\"{row_ct} records inserted\")\n",
    "\n",
    "print(\"All records inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #daily_subregion_data\n",
    "# Create the table (if it doesn't exist)\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_subregion (\n",
    "    period VARCHAR(20),\n",
    "    subba VARCHAR(20),\n",
    "    subba_name VARCHAR(200),\n",
    "    parent VARCHAR(20),\n",
    "    parent_name VARCHAR(50),\n",
    "    timezone VARCHAR(20),\n",
    "    value VARCHAR(255),  # Set as VARCHAR to avoid range issues\n",
    "    value_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('daily_subregion_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare the data for bulk insertion as a list of tuples\n",
    "insert_data = []\n",
    "for record in data:\n",
    "    value = record.get(\"value\")\n",
    "    \n",
    "    # Add record to insert_data without casting to float\n",
    "    insert_data.append((\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"subba\"),\n",
    "        record.get(\"subba-name\"),\n",
    "        record.get(\"parent\"),\n",
    "        record.get(\"parent-name\"),\n",
    "        record.get(\"timezone\"),\n",
    "        value,\n",
    "        record.get(\"value-units\")\n",
    "    ))\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 500  # Adjust this size as needed\n",
    "\n",
    "# Insert data in batches and handle errors\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO daily_subregion (\n",
    "    period, subba, subba_name, parent, parent_name, \n",
    "    timezone, value, value_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "total_records = len(insert_data)\n",
    "records_inserted = 0\n",
    "\n",
    "for i in range(0, total_records, batch_size):\n",
    "    batch = insert_data[i:i + batch_size]\n",
    "    try:\n",
    "        cursor.executemany(insert_query, batch)\n",
    "        connection.commit()  # Commit after each batch to avoid locking issues\n",
    "        records_inserted += len(batch)\n",
    "        print(f\"{records_inserted}/{total_records} records inserted\")\n",
    "    except Exception as e:\n",
    "        # Print each problematic value in the batch\n",
    "        print(\"Error encountered:\", e)\n",
    "        for record in batch:\n",
    "            try:\n",
    "                cursor.execute(insert_query, record)  # Try individual insertion\n",
    "            except Exception as single_error:\n",
    "                print(f\"Problematic value for record {record}: {single_error}\")\n",
    "\n",
    "print(\"All valid records inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create table query for daily_generation_data\n",
    "# Drop the table if it exists\n",
    "drop_table_query = \"DROP TABLE IF EXISTS daily_generation;\"\n",
    "cursor.execute(drop_table_query)\n",
    "\n",
    "# Create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_generation (\n",
    "    period VARCHAR(20),\n",
    "    respondent VARCHAR(20),\n",
    "    respondent_name VARCHAR(200),\n",
    "    fueltype VARCHAR(10),\n",
    "    type_name VARCHAR(50),\n",
    "    timezone VARCHAR(20),\n",
    "    timezone_description VARCHAR(50),\n",
    "    value DECIMAL(10, 2),\n",
    "    value_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('daily_generation_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare the data for bulk insertion as a list of tuples\n",
    "insert_data = [\n",
    "    (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"respondent\"),\n",
    "        record.get(\"respondent-name\"),\n",
    "        record.get(\"fueltype\"),\n",
    "        record.get(\"type-name\"),\n",
    "        record.get(\"timezone\"),\n",
    "        record.get(\"timezone-description\"),\n",
    "        float(record.get(\"value\") or 0),\n",
    "        record.get(\"value-units\")\n",
    "    )\n",
    "    for record in data\n",
    "]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 500  # Adjust this size as needed\n",
    "\n",
    "# Insert data in batches using executemany\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO daily_generation (\n",
    "    period, respondent, respondent_name, fueltype, type_name, \n",
    "    timezone, timezone_description, value, value_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Initialize row counter\n",
    "row_ct = 0\n",
    "\n",
    "# Insert data in batches\n",
    "for i in range(0, len(insert_data), batch_size):\n",
    "    batch = insert_data[i:i + batch_size]\n",
    "    cursor.executemany(insert_query, batch)\n",
    "    connection.commit()  # Commit after each batch to avoid locking issues\n",
    "    \n",
    "    # Update and print the row count\n",
    "    row_ct += len(batch)\n",
    "    print(f\"{row_ct} records inserted\")\n",
    "\n",
    "print(\"All records inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the table if it exists\n",
    "drop_table_query = \"DROP TABLE IF EXISTS daily_interchange;\"\n",
    "cursor.execute(drop_table_query)\n",
    "\n",
    "# Create the table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_interchange (\n",
    "    period VARCHAR(20),\n",
    "    fromba VARCHAR(20),\n",
    "    fromba_name VARCHAR(200),\n",
    "    toba VARCHAR(20),\n",
    "    toba_name VARCHAR(200),\n",
    "    timezone VARCHAR(20),\n",
    "    value DECIMAL(10, 2),\n",
    "    value_units VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('interchange_nba_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare the data for bulk insertion as a list of tuples\n",
    "insert_data = [\n",
    "    (\n",
    "        record.get(\"period\"),\n",
    "        record.get(\"fromba\"),\n",
    "        record.get(\"fromba-name\"),\n",
    "        record.get(\"toba\"),\n",
    "        record.get(\"toba-name\"),\n",
    "        record.get(\"timezone\"),\n",
    "        float(record.get(\"value\") or 0),\n",
    "        record.get(\"value-units\")\n",
    "    )\n",
    "    for record in data\n",
    "]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 500  # Adjust this size as needed\n",
    "\n",
    "# Insert query for batch insertion\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO daily_interchange (\n",
    "    period, fromba, fromba_name, toba, toba_name, \n",
    "    timezone, value, value_units\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Initialize row counter\n",
    "row_ct = 0\n",
    "\n",
    "# Insert data in batches\n",
    "for i in range(0, len(insert_data), batch_size):\n",
    "    batch = insert_data[i:i + batch_size]\n",
    "    cursor.executemany(insert_query, batch)\n",
    "    connection.commit()  # Commit after each batch to avoid locking issues\n",
    "    \n",
    "    # Update and print the row count\n",
    "    row_ct += len(batch)\n",
    "    print(f\"{row_ct} records inserted\")\n",
    "\n",
    "print(\"All records inserted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_query = \"DROP TABLE IF EXISTS _misc_ultimate_customers;\"\n",
    "cursor.execute(drop_table_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Data Engineering Portion Before the Forecasting and Visualization Stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload of CSV Files Which Store Forecasting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Be Performed After Running the Forecasting Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        # Establishing the connection\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            password=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "        if connection.is_connected():\n",
    "            print(\"Connection to MySQL DB successful\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "    \n",
    "    return connection\n",
    "\n",
    "def close_connection(connection):\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"The connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL DB successful\n",
      "Connected to database: ('testing',)\n"
     ]
    }
   ],
   "source": [
    "# DO NOT UNCOMMENT; WILL AFFECT ENTIRE DB:\n",
    "# host_name = \"34.23.64.152\" \n",
    "# user_name = \"root\"  \n",
    "# user_password = \"cse6242\" \n",
    "# db_name = \"testing\" \n",
    "connection = create_connection(host_name, user_name, user_password, db_name)\n",
    "if connection:\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT DATABASE();\")\n",
    "    result = cursor.fetchone()\n",
    "    print(f\"Connected to database: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table query\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS rmse_comparison (\n",
    "    respondent VARCHAR(10),\n",
    "    demand_model VARCHAR(20),\n",
    "    net_generation_model VARCHAR(20)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# File path to CSV\n",
    "csv_file_path = 'rmse_comparison_summary_best_model.csv'  # Replace with your actual file path\n",
    "\n",
    "# Insert query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO rmse_comparison (respondent, demand_model, net_generation_model)\n",
    "VALUES (%s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Batch size\n",
    "batch_size = 100  # Adjust based on your needs\n",
    "batch = []\n",
    "count = 0\n",
    "try:\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        for row in csv_reader:\n",
    "            # Add to batch\n",
    "            batch.append((\n",
    "                row['respondent'],\n",
    "                row['demand'] if row['demand'] else None,\n",
    "                row['net generation'] if row['net generation'] else None\n",
    "            ))\n",
    "            \n",
    "            # When batch reaches the defined size, execute it\n",
    "            if len(batch) >= batch_size:\n",
    "                cursor.executemany(insert_query, batch)\n",
    "                connection.commit()  # Commit the batch\n",
    "                batch = []  # Clear the batch\n",
    "\n",
    "        # Insert any remaining rows\n",
    "        if batch:\n",
    "            cursor.executemany(insert_query, batch)\n",
    "            connection.commit()\n",
    "            count += batch_size\n",
    "            print(count)\n",
    "            \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close connections\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table query\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS merged_predictions (\n",
    "    respondent VARCHAR(10),\n",
    "    type_name VARCHAR(20),\n",
    "    period DATETIME,\n",
    "    actuals DECIMAL(10, 2),\n",
    "    day_ahead_demand_forecast DOUBLE,\n",
    "    best_prediction DOUBLE,\n",
    "    within_3_std BOOLEAN\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# File path to CSV\n",
    "csv_file_path = 'merged_predictions_cleaned.csv'  # Replace with your actual file path\n",
    "\n",
    "# Insert query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO merged_predictions (\n",
    "    respondent, type_name, period, actuals, day_ahead_demand_forecast, best_prediction, within_3_std\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Batch size\n",
    "batch_size = 5000  # Adjust based on your needs\n",
    "batch = []\n",
    "count = 0\n",
    "\n",
    "try:\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        for row in csv_reader:\n",
    "            try:\n",
    "                # Add to batch\n",
    "                record = (\n",
    "                    row['respondent'],\n",
    "                    row['type_name'],\n",
    "                    row['period'],\n",
    "                    float(row['actuals']) if row['actuals'] else None,\n",
    "                    float(row['day_ahead_demand_forecast']) if row['day_ahead_demand_forecast'] else None,\n",
    "                    float(row['best_prediction']) if row['best_prediction'] else None,\n",
    "                    bool(row['within_3_std']) if row['within_3_std'] else None\n",
    "                )\n",
    "                batch.append(record)\n",
    "            except Exception as row_error:\n",
    "                print(f\"Error processing row: {row}\")\n",
    "                print(f\"Row Error: {row_error}\")\n",
    "\n",
    "            # When batch reaches the defined size, execute it\n",
    "            if len(batch) >= batch_size:\n",
    "                try:\n",
    "                    cursor.executemany(insert_query, batch)\n",
    "                    connection.commit()  # Commit the batch\n",
    "                    count += len(batch)  # Track inserted rows\n",
    "                    print(f\"Inserted {count} rows...\")\n",
    "                    batch = []  # Clear the batch\n",
    "                except Exception as batch_error:\n",
    "                    print(f\"Error in batch insertion: {batch_error}\")\n",
    "                    print(\"Isolating problematic rows...\")\n",
    "\n",
    "                    # Isolate problematic row within the batch\n",
    "                    for problematic_row in batch:\n",
    "                        try:\n",
    "                            cursor.execute(insert_query, problematic_row)\n",
    "                        except Exception as row_error:\n",
    "                            print(f\"Problematic row: {problematic_row}\")\n",
    "                            print(f\"Row Error: {row_error}\")\n",
    "                    batch = []  # Clear the problematic batch\n",
    "\n",
    "        # Insert any remaining rows\n",
    "        if batch:\n",
    "            try:\n",
    "                cursor.executemany(insert_query, batch)\n",
    "                connection.commit()\n",
    "                count += len(batch)\n",
    "                print(f\"Inserted {count} rows...\")\n",
    "            except Exception as batch_error:\n",
    "                print(f\"Error in final batch insertion: {batch_error}\")\n",
    "                print(\"Isolating problematic rows...\")\n",
    "\n",
    "                # Isolate problematic row within the final batch\n",
    "                for problematic_row in batch:\n",
    "                    try:\n",
    "                        cursor.execute(insert_query, problematic_row)\n",
    "                    except Exception as row_error:\n",
    "                        print(f\"Problematic row: {problematic_row}\")\n",
    "                        print(f\"Row Error: {row_error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
